projectId: your-gcp-project-id
datasetExternal: raw_init
datasetFinal: dwh
gcsBucket: your-destination-bucket
# AKIAZ76EFTFHXOUJSAG2    NHHNORi+rhKFTHBSbahOFzORVNa/kq6eKHP9peF8

# When set to `auto`, the pipeline chooses STS by default and falls back to
# Dataflow or Dataproc based on your thresholds.  You can override per
# table.  Valid values: auto | sts | dataflow | dataproc
defaultEngine: auto

tables:
- name: member_address
  source:
    s3Bucket: demo-t1-analytics
    prefix: emr/member_address/
    format: parquet
    # schemaSource: glue
    # glue:
    #   database: dwh
    #   table: member_address
  destination:
    gcsPrefix: landing/member_address/
    biglake:
      hivePartitioning: true
      hivePartitionUriPrefix: gs://your-destination-bucket/landing/member_address/
    finalTable:
      dataset: dwh
      table: member_address
    columnMapping:
    # map expressions to destination columns
    - expr: order_id
      as: order_id
    - expr: cast(amount as numeric)
      as: amount
    - expr: dt
      as: dt
  engineOverrides:
    preferred: sts
    fallbacks:
    - dataflow
    - dataproc
  thresholds:
    maxSingleObjectTiB: 5
    minCompactTargetMB: 256
# Add more tables here as needed

validation:
  compareWith: s3 # s3 | redshift
  checksumColumns: [ "order_id", "amount", "dt" ]
  sampleRatio: 0.0 # 0 = no sampling, >0 = sample random records
